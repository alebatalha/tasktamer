# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HDM2SqpfRjx5GXmeLyN3oSWS2QcO0KNk
"""

!pip install farm-haystack

!apt-get install -y poppler-utils

# TaskTamer Core Implementation

from haystack import Pipeline
from haystack.nodes import PromptNode, PromptTemplate
from haystack.document_stores import InMemoryDocumentStore
from haystack.nodes import PreProcessor, PDFToTextConverter, TextConverter
from haystack.utils import fetch_archive_from_http
from typing import List, Dict, Union
import os
import requests
from bs4 import BeautifulSoup

# Initialize Document Store
document_store = InMemoryDocumentStore()

# Preprocessing Module
preprocessor = PreProcessor(
    clean_empty_lines=True,
    clean_whitespace=True,
    split_by='word',
    split_length=200,
    split_overlap=50,
    split_respect_sentence_boundary=True
)

# Summarization & Question Generation
summary_prompt = PromptNode(
    model_name_or_path="google/flan-t5-large",
    default_prompt_template=PromptTemplate(
        "Summarize the following document: {documents}"
    )
)

question_prompt = PromptNode(
    model_name_or_path="google/flan-t5-large",
    default_prompt_template=PromptTemplate(
        "Generate a multiple-choice question with one correct answer and three incorrect alternatives from: {documents}"
    )
)

def break_task(task_description: str) -> List[str]:
    """Breaks a task into actionable steps."""
    prompt = f"Break the following task into smaller steps: {task_description}"
    response = summary_prompt([prompt])  # Pass a list of strings

    if isinstance(response, dict) and "results" in response:
        return response["results"][0].split("\n")
    return ["Error: No response received."]

# Function to fetch and process web pages
def fetch_webpage_content(url: str) -> str:
    """Fetches and extracts text content from a web page."""
    try:
        response = requests.get(url)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")
        paragraphs = soup.find_all("p")
        text = "\n".join([p.get_text() for p in paragraphs])
        return text if text else "No readable content found."
    except requests.RequestException as e:
        return f"Error fetching webpage: {e}"

import os

def process_documents(files_or_urls: List[str]):
    """Processes multiple document types including web pages."""
    if not files_or_urls:
        return "No document or URL provided. Please upload a document or enter a webpage URL."

    for item in files_or_urls:
        if item.startswith("http"):
            raw_text = fetch_webpage_content(item)
        elif os.path.exists(item):  # Check if file exists before processing
            if item.endswith(".pdf"):
                converter = PDFToTextConverter()
                raw_text = converter.convert(file_path=item)
                if isinstance(raw_text, list) and raw_text:
                    raw_text = raw_text[0].content  # Extract text from Document object
                else:
                    raw_text = ""
            else:
                converter = TextConverter()
                raw_text = converter.convert(file_path=item)
                if isinstance(raw_text, list) and raw_text:
                    raw_text = raw_text[0].content
                else:
                    raw_text = ""
        else:
            print(f"Warning: File '{item}' not found. Skipping.")
            continue  # Skip to the next file

        if raw_text:
            processed_docs = preprocessor.process([{"content": raw_text}])
            document_store.write_documents(processed_docs)
        else:
            print(f"Warning: No text extracted from {item}")

# Summarization Function
def summarize_documents():
    """Summarizes stored documents."""
    docs = document_store.get_all_documents()
    if not docs:
        return "No document found. Please upload a document or enter a webpage URL first."
    return summary_prompt(documents=docs)

# Question Generation Function
def generate_questions():
    """Generates study questions from stored documents."""
    docs = document_store.get_all_documents()
    if not docs:
        return "No document found. Please upload a document or enter a webpage URL first."
    return question_prompt(documents=docs)

# Placeholder for Obsidian Integration
def export_to_obsidian(notes: List[Dict[str, str]], approve: bool):
    """Exports user-selected notes to Obsidian if approved."""
    if approve:
        for note in notes:
            with open(f"ObsidianVault/{note['title']}.md", "w") as f:
                f.write(note['content'])

# Example Usage
if __name__ == "__main__":
    task = "Write a research paper on AI ethics."
    print("Task Breakdown:", break_task(task))

    process_documents(["sample.pdf", "notes.txt"])
    print("Summary:", summarize_documents())
    print("Generated Questions:", generate_questions())

test_url = "https://en.wikipedia.org/wiki/Artificial_intelligence"  # Example webpage
process_documents([test_url])

print("Summary:", summarize_documents())
print("Generated Questions:", generate_questions())
